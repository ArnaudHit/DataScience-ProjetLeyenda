{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Clasification model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca17ca008d5e8ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b849f61603eb0ae"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:28.395905600Z",
     "start_time": "2023-10-26T09:29:25.537430700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 09:29:25.781657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-26 09:29:25.781715: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-26 09:29:25.781776: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-26 09:29:25.790248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import \"classification_model.keras\" from root"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d08afdd1360e1840"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"./photo/Photo\")\n",
    "model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "classifier_path = os.path.join(model_dir, \"classifier.keras\")\n",
    "denoiser_path = os.path.join(model_dir, \"denoiser.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:28.395905600Z",
     "start_time": "2023-10-26T09:29:28.287738300Z"
    }
   },
   "id": "391a9eb35f5dab76"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 09:29:28.344809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-26 09:29:28.349133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-26 09:29:28.349172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-26 09:29:28.349686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-26 09:29:28.349717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-26 09:29:28.349755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-26 09:29:28.990457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-26 09:29:28.990548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-26 09:29:28.990557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-26 09:29:28.990591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-26 09:29:28.990609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6599 MB memory:  -> device: 0, name: Quadro P4000, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.keras.models.load_model(classifier_path)\n",
    "denoiser = tf.keras.models.load_model(denoiser_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:30.293314Z",
     "start_time": "2023-10-26T09:29:28.292619Z"
    }
   },
   "id": "7ec0bbeef8f82c2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9939a9e38066de"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "    contenant l'image traitée ainsi que son chemin d'accès.\n",
    "    La fonction load_image effectue les traitement suivant:\n",
    "        1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "        2. Décodage de l'image en RGB.\n",
    "        3. Redimensionnement de l'image en taille (299, 299).\n",
    "        4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "    \"\"\"\n",
    "    # Lecture du fichier image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    # Decodage de l'image en RGB\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Redimensionnement de l'image en taille (299, 299)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    # Normalisation des pîxels de l'image entre -1 et 1\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:30.294122100Z",
     "start_time": "2023-10-26T09:29:30.287970800Z"
    }
   },
   "id": "73f0e6723af2ff06"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 512 # Taille de la couche cachée dans le RNN\n",
    "top_k = 5000 # Nombre de mots à utiliser dans le vocabulaire\n",
    "vocab_size = top_k + 1\n",
    "max_length = 47 # Nombre maximum de mots dans une légende\n",
    "attention_features_shape = 64 # Taille de la couche d'attention"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:30.312601Z",
     "start_time": "2023-10-26T09:29:30.295212100Z"
    }
   },
   "id": "caf96743ce1cbf69"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "    # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # forme après fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:30.345370100Z",
     "start_time": "2023-10-26T09:29:30.315454300Z"
    }
   },
   "id": "b8f571a81346794f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) forme == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # forme de la couche cachée == (batch_size, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # Cela vous donne un score non normalisé pour chaque caractéristique de l'image.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        #Couche dense qui aura pour entrée la sortie du GRU\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        # Dernière couche dense\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # L'attention est defini par un modèle a part\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # Passage du mot courant à la couche embedding\n",
    "        x = self.embedding(x)\n",
    "        # Concaténation\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Passage du vecteur concaténé à la gru\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # Couche dense\n",
    "        y = self.fc1(output)\n",
    "\n",
    "        y = tf.reshape(y, (-1, x.shape[2]))\n",
    "\n",
    "        # Couche dense\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:30.354878800Z",
     "start_time": "2023-10-26T09:29:30.333739900Z"
    }
   },
   "id": "8c8ddf23d4be1a89"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Création de l'encodeur\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "# Création du décodeur\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:30.385710400Z",
     "start_time": "2023-10-26T09:29:30.345370100Z"
    }
   },
   "id": "74abab2076f7113b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.abspath(\"./models/cp/\")  # Modifiez ce chemin si nécessaire.\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=tf.keras.optimizers.Adam())  # Assurez-vous d'utiliser les mêmes paramètres d'optimiseur que l'original.\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:30.398088200Z",
     "start_time": "2023-10-26T09:29:30.391909700Z"
    }
   },
   "id": "fccdbb0f8aaabadb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle de captioning chargé !\n"
     ]
    }
   ],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Modèle de captioning chargé !\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:30.425870800Z",
     "start_time": "2023-10-26T09:29:30.397575900Z"
    }
   },
   "id": "8595a413838e703b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "    contenant l'image traitée ainsi que son chemin d'accès.\n",
    "    La fonction load_image effectue les traitement suivant:\n",
    "        1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "        2. Décodage de l'image en RGB.\n",
    "        3. Redimensionnement de l'image en taille (299, 299).\n",
    "        4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "    \"\"\"\n",
    "    # Lecture du fichier image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    # Decodage de l'image en RGB\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Redimensionnement de l'image en taille (299, 299)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    # Normalisation des pîxels de l'image entre -1 et 1\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:30.427058800Z",
     "start_time": "2023-10-26T09:29:30.421001100Z"
    }
   },
   "id": "4f1f829da05ba78"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "# Creation d'une variable qui sera l'entrée du nouveau modèle de pre-traitement d'images\n",
    "new_input = image_model.input\n",
    "# récupérer la dernière couche caché qui contient l'image en representation compacte\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "# Modèle qui calcule une representation dense des images avec InceptionV3\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:32.486341400Z",
     "start_time": "2023-10-26T09:29:30.427599Z"
    }
   },
   "id": "5c42484915583b77"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:32.500742200Z",
     "start_time": "2023-10-26T09:29:32.495437900Z"
    }
   },
   "id": "81f71cf2985004b9"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Load the JSON string from the file\n",
    "with open('tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "\n",
    "# Recreate the tokenizer from the JSON string\n",
    "tokenizer = tokenizer_from_json(tokenizer_json)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:32.538821400Z",
     "start_time": "2023-10-26T09:29:32.502416900Z"
    }
   },
   "id": "439cf238ca16d5d7"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Fonction permettant la représentation de l'attention au niveau de l'image\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:29:32.549209800Z",
     "start_time": "2023-10-26T09:29:32.542714500Z"
    }
   },
   "id": "22cefb442f0376ef"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# if image is photo, go onto second model\u001B[39;00m\n\u001B[1;32m     12\u001B[0m tf_img \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mpreprocessing\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mload_img(image_path, target_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m))\n\u001B[0;32m---> 13\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf_img\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m score \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39msoftmax(pred[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m score[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m:\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m# denoise image\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:2627\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   2625\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution_tuner\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m   2626\u001B[0m batch_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2627\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, iterator \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39menumerate_epochs():  \u001B[38;5;66;03m# Single epoch.\u001B[39;00m\n\u001B[1;32m   2628\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[1;32m   2629\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39msteps():\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/engine/data_adapter.py:1341\u001B[0m, in \u001B[0;36mDataHandler.enumerate_epochs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1339\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001B[39;00m\n\u001B[1;32m   1340\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_truncate_execution_to_epoch():\n\u001B[0;32m-> 1341\u001B[0m     data_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset)\n\u001B[1;32m   1342\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_initial_epoch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_epochs):\n\u001B[1;32m   1343\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_insufficient_data:  \u001B[38;5;66;03m# Set by `catch_stop_iteration`.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py:496\u001B[0m, in \u001B[0;36mDatasetV2.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly() \u001B[38;5;129;01mor\u001B[39;00m ops\u001B[38;5;241m.\u001B[39minside_function():\n\u001B[1;32m    495\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mcolocate_with(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variant_tensor):\n\u001B[0;32m--> 496\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43miterator_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mOwnedIterator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    498\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.Dataset` only supports Python-style \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    499\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miteration in eager mode or within tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py:705\u001B[0m, in \u001B[0;36mOwnedIterator.__init__\u001B[0;34m(self, dataset, components, element_spec)\u001B[0m\n\u001B[1;32m    701\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m (components \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m element_spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    702\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    703\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    704\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot be specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 705\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    707\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_next_call_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001B[0m, in \u001B[0;36mOwnedIterator._create_iterator\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    741\u001B[0m   \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fulltype\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39margs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(\n\u001B[1;32m    742\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_output_types)\n\u001B[1;32m    743\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator_resource\u001B[38;5;241m.\u001B[39mop\u001B[38;5;241m.\u001B[39mexperimental_set_type(fulltype)\n\u001B[0;32m--> 744\u001B[0m \u001B[43mgen_dataset_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds_variant\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator_resource\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py:3420\u001B[0m, in \u001B[0;36mmake_iterator\u001B[0;34m(dataset, iterator, name)\u001B[0m\n\u001B[1;32m   3418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[1;32m   3419\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3420\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3421\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMakeIterator\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3422\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m   3423\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Predict on first model, if it is a photo, pass on second model to denoise image, and then pass on third model to get caption\n",
    "# list of new images & captions\n",
    "list_of_images = []\n",
    "list_of_captions = []\n",
    "list_of_attention = []\n",
    "\n",
    "image_file = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir) if filename.endswith('.jpg')]\n",
    "\n",
    "for img in image_file:\n",
    "    image_path = os.path.join(data_dir, img)\n",
    "    # if image is photo, go onto second model\n",
    "    tf_img = tf.keras.preprocessing.image.load_img(image_path, target_size=(128, 128))\n",
    "    pred = classifier.predict(tf.expand_dims(tf_img, 0))\n",
    "    score = tf.nn.softmax(pred[0])\n",
    "    if score[0] > 0.5:\n",
    "        # denoise image\n",
    "        image = denoiser.predict(tf.expand_dims(tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256)), 0))\n",
    "        # numpy array to img\n",
    "        ### DJAYAN ICI STP\n",
    "        ### En gros il faut renvoyer une IMAGE à la fonction evaluate\n",
    "        #image = tf.keras.preprocessing.image.array_to_img(image)\n",
    "        # get caption\n",
    "        result, attention_plot = evaluate(img)\n",
    "        # remove <start> and <end> from caption\n",
    "        result = result[1:-1]\n",
    "        # convert caption from list to string\n",
    "        caption = ' '.join(result)\n",
    "        # append image, caption, and attention to list\n",
    "        list_of_images.append(img)\n",
    "        list_of_captions.append(caption)\n",
    "        list_of_attention.append(attention_plot)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T09:52:51.313980400Z",
     "start_time": "2023-10-26T09:52:47.412681800Z"
    }
   },
   "id": "39e41bcff41bfa94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e807b69ef3facf2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
