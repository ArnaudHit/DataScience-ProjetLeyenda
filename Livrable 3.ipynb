{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10586ddc4e0417b6"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.3.2)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.11.3)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.8.0)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.2.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.1.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.42.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (23.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.0.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.66.1)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn scipy matplotlib\n",
    "!pip install tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import zipfile\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:31:01.540355800Z",
     "start_time": "2023-10-25T09:30:57.983935800Z"
    }
   },
   "id": "5fca277094268aa0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chargement des données"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d8f41b0cfff98d4"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[92], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m zipfile\u001B[38;5;241m.\u001B[39mis_zipfile(chemin_fichier):\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m zipfile\u001B[38;5;241m.\u001B[39mZipFile(chemin_fichier, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m zip_ref:\n\u001B[0;32m---> 10\u001B[0m             \u001B[43mzip_ref\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextractall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdossier_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFichier \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfichier\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m décompressé dans le dossier Dataset.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTous les fichiers ZIP ont été décompressés dans le dossier Dataset\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/lib/python3.11/zipfile.py:1677\u001B[0m, in \u001B[0;36mZipFile.extractall\u001B[0;34m(self, path, members, pwd)\u001B[0m\n\u001B[1;32m   1674\u001B[0m     path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mfspath(path)\n\u001B[1;32m   1676\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m zipinfo \u001B[38;5;129;01min\u001B[39;00m members:\n\u001B[0;32m-> 1677\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_extract_member\u001B[49m\u001B[43m(\u001B[49m\u001B[43mzipinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpwd\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.11/zipfile.py:1732\u001B[0m, in \u001B[0;36mZipFile._extract_member\u001B[0;34m(self, member, targetpath, pwd)\u001B[0m\n\u001B[1;32m   1728\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m targetpath\n\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopen(member, pwd\u001B[38;5;241m=\u001B[39mpwd) \u001B[38;5;28;01mas\u001B[39;00m source, \\\n\u001B[1;32m   1731\u001B[0m      \u001B[38;5;28mopen\u001B[39m(targetpath, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m target:\n\u001B[0;32m-> 1732\u001B[0m     \u001B[43mshutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopyfileobj\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1734\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m targetpath\n",
      "File \u001B[0;32m/usr/lib/python3.11/shutil.py:197\u001B[0m, in \u001B[0;36mcopyfileobj\u001B[0;34m(fsrc, fdst, length)\u001B[0m\n",
      "File \u001B[0;32m/usr/lib/python3.11/zipfile.py:951\u001B[0m, in \u001B[0;36mZipExtFile.read\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m    949\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_offset \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    950\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m n \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eof:\n\u001B[0;32m--> 951\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    952\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(data):\n\u001B[1;32m    953\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_readbuffer \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[0;32m/usr/lib/python3.11/zipfile.py:1021\u001B[0m, in \u001B[0;36mZipExtFile._read1\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m   1019\u001B[0m         data \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read2(n \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(data))\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1021\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compress_type \u001B[38;5;241m==\u001B[39m ZIP_STORED:\n\u001B[1;32m   1024\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eof \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compress_left \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m/usr/lib/python3.11/zipfile.py:1051\u001B[0m, in \u001B[0;36mZipExtFile._read2\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m   1048\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(n, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMIN_READ_SIZE)\n\u001B[1;32m   1049\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(n, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compress_left)\n\u001B[0;32m-> 1051\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fileobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1052\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compress_left \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data)\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n",
      "File \u001B[0;32m/usr/lib/python3.11/zipfile.py:771\u001B[0m, in \u001B[0;36m_SharedFile.read\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m    767\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt read from the ZIP file while there \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    768\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis an open writing handle on it. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    769\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClose the writing handle before trying to read.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    770\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos)\n\u001B[0;32m--> 771\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file\u001B[38;5;241m.\u001B[39mread(n)\n\u001B[1;32m    772\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file\u001B[38;5;241m.\u001B[39mtell()\n\u001B[1;32m    773\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[0;31mOSError\u001B[0m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "repertoire = './Train2014Zip'\n",
    "dossier_dataset = './train2014'\n",
    "if not os.path.exists(dossier_dataset):\n",
    "    os.makedirs(dossier_dataset)\n",
    "\n",
    "for fichier in os.listdir(repertoire):\n",
    "    chemin_fichier = os.path.join(repertoire,fichier)\n",
    "    if zipfile.is_zipfile(chemin_fichier):\n",
    "        with zipfile.ZipFile(chemin_fichier, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dossier_dataset)\n",
    "            print(f'Fichier {fichier} décompressé dans le dossier Dataset.')\n",
    "print('Tous les fichiers ZIP ont été décompressés dans le dossier Dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:36:02.803253400Z",
     "start_time": "2023-10-25T09:31:01.547687400Z"
    }
   },
   "id": "6741f00eb709c661"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Chemin du fichier d'annotations\n",
    "annotation_folder = os.path.abspath('.')+\"/annotations/\"\n",
    "annotation_file = annotation_folder+\"captions_train2014.json\"\n",
    "\n",
    "# Chemin du dossier contenant les images à annoter\n",
    "image_folder = '/train2014/train2014/'\n",
    "PATH = os.path.abspath('.') + image_folder\n",
    "\n",
    "print(\"annotation file \", annotation_file)\n",
    "print(\"le chemin PATH \", PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:36:02.820462300Z",
     "start_time": "2023-10-25T09:36:02.812134500Z"
    }
   },
   "id": "8771aced2726b0d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(annotation_folder+\"/captions_val2014.json\", 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "with open(annotation_folder+\"instances_val2014.json\", 'r') as f:\n",
    "    instances = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.812649700Z"
    }
   },
   "id": "dfee0dd43940945d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lecture du fichier d'annotation\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Grouper toutes les annotations ayant le meme identifiant.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    # marquer le debut et la fin de chaque annotation\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    # L'identifiant d'une image fait partie de son chemin d'accès\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    # Rajout du caption associé à image_path\n",
    "    image_path_to_caption[image_path].append(caption)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.816760600Z"
    }
   },
   "id": "22bc87c73615b55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prendre les premières images seulement\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "train_image_paths = image_paths[:10000]\n",
    "print(len(train_image_paths))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.819459200Z"
    }
   },
   "id": "886e2d5050af6a69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "On affiche un exemple d'image et de ses annotations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63e5aa86d6a05cfc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Liste de toutes les annotations\n",
    "train_captions = []\n",
    "# Liste de tous les noms de fichiers des images dupliquées (en nombre d'annotations par image)\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    # Rajout de caption_list dans train_captions\n",
    "    train_captions.extend(caption_list)\n",
    "    # Rajout de image_path dupliquée len(caption_list) fois\n",
    "    img_name_vector.extend([image_path] * len(caption_list))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.821937600Z"
    }
   },
   "id": "ca877999a1d3177b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(train_captions), len(img_name_vector))\n",
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.823502600Z"
    }
   },
   "id": "7d2585afaa85f28d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Prétraitement des images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61b4185d8a144ae4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Telechargement du modèle InceptionV3 pré-entrainé avec la cassification sur ImageNet\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "# Creation d'une variable qui sera l'entrée du nouveau modèle de pre-traitement d'images\n",
    "new_input = image_model.input\n",
    "# récupérer la dernière couche caché qui contient l'image en representation compacte\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "# Modèle qui calcule une representation dense des images avec InceptionV3\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.826852400Z"
    }
   },
   "id": "9e6e791ab93361c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "    contenant l'image traitée ainsi que son chemin d'accès.\n",
    "    La fonction load_image effectue les traitement suivant:\n",
    "        1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "        2. Décodage de l'image en RGB.\n",
    "        3. Redimensionnement de l'image en taille (299, 299).\n",
    "        4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "    \"\"\"\n",
    "    # Lecture du fichier image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    # Decodage de l'image en RGB\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Redimensionnement de l'image en taille (299, 299)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    # Normalisation des pîxels de l'image entre -1 et 1\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.828120600Z"
    }
   },
   "id": "872b34652ff4aea9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Pré-traitement des images\n",
    "# Prendre les noms des images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Creation d'une instance de \"tf.data.Dataset\" partant des noms des images \n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "# Division du données en batchs après application du pré-traitement fait par load_image\n",
    "image_dataset = image_dataset.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "# Parcourir le dataset batch par batch pour effectuez le pré-traitement d'InceptionV3\n",
    "for img, path in tqdm(image_dataset):\n",
    "    # Pré-traitement du batch (de taille (16,8,8,2048)) courant par InceptionV3 \n",
    "    batch_features = image_features_extract_model(img)\n",
    "    # Resize du batch de taille (16,8,8,2048) en taille (16,64,2048)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                                (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    # Parcourir le batch courant et stocker le chemin ainsi que le batch avec np.save()\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # (chemin de l'image associe a sa nouvelle representation , representation de l'image)\n",
    "        np.save(\n",
    "            path_of_feature, bf.numpy()\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.829862700Z"
    }
   },
   "id": "253f8c1d8acdf094"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Prétraitement des annotations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10ef24d8e5d6e205"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trouver la taille maximale \n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Chosir les 5000 mots les plus frequents du vocabulaire\n",
    "top_k = 5000\n",
    "#La classe Tokenizer permet de faire du pre-traitement de texte pour reseau de neurones \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "# Construit un vocabulaire en se basant sur la liste train_captions\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "# Créer le token qui sert à remplir les annotations pour egaliser leurs longueur\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Creation des vecteurs(liste de token entiers) à partir des annotations (liste de mots)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Remplir chaque vecteur à jusqu'à la longueur maximale des annotations\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calcule la longueur maximale qui est utilisée pour stocker les poids d'attention \n",
    "# Elle servira plus tard pour l'affichage lors de l'évaluation\n",
    "max_length = calc_max_length(train_seqs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:36:02.841904900Z",
     "start_time": "2023-10-25T09:36:02.835356800Z"
    }
   },
   "id": "43d222e2ce67122a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Formation du jeu d'entrainement et de test :"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3643fcb93d42662d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "# Creation d'un dictionnaire associant les chemins des images avec (fichier .npy) aux annotationss\n",
    "# Les images sont dupliquées car il y a plusieurs annotations par image\n",
    "print(len(img_name_vector), len(cap_vector))\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "\"\"\"\n",
    "Création des datasets de formation et de validation en utilisant \n",
    "un fractionnement 80-20 de manière aléatoire\n",
    "\"\"\"\n",
    "# Prendre les clés (noms des fichiers d'images traites), *celles-ci ne seront pas dupliquées*\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys) # Mélanger les clés\n",
    "# Diviser des indices en entrainement et test\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "# divide into train and validation\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "\"\"\"\n",
    "Les jeux d'entrainement et de tests sont sous forme\n",
    "de listes contenants les mappings :(image prétraitée ---> jeton d'annotation(mot) )\n",
    "\"\"\"\n",
    "\n",
    "# Boucle pour construire le jeu d'entrainement\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    # Duplication des images en le nombre d'annotations par image\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "# Boucle pour construire le jeu de test\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    # Duplication des images en le nombre d'annotations par image\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.839030200Z"
    }
   },
   "id": "a0a9a719b9185c41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# N'hésitez pas à modifier ces paramètres en fonction de votre machine\n",
    "BATCH_SIZE = 64 # taille du batch\n",
    "BUFFER_SIZE = 1000 # taille du buffer pour melanger les donnes\n",
    "embedding_dim = 256\n",
    "units = 512 # Taille de la couche caché dans le RNN\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# La forme du vecteur extrait à partir d'InceptionV3 est (64, 2048)\n",
    "# Les deux variables suivantes representent la forme de ce vecteur\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "# Fonction qui charge les fichiers numpy des images prétraitées\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "# Creation d'un dataset de \"Tensor\"s (sert à representer de grands dataset)\n",
    "# Le dataset est cree a partir de \"img_name_train\" et \"cap_train\"\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# L'utilisation de map permet de charger les fichiers numpy (possiblement en parallèle)\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "    map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Melanger les donnees et les diviser en batchs\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.840144300Z"
    }
   },
   "id": "3ff2a5c9f3f44e50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Le modèle :"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c43a3b6b7bf5aee0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "    # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # forme après fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:36:02.842439600Z",
     "start_time": "2023-10-25T09:36:02.841904900Z"
    }
   },
   "id": "3b10b31ce60a1025"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) forme == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # forme de la couche cachée == (batch_size, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # Cela vous donne un score non normalisé pour chaque caractéristique de l'image.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.843488400Z"
    }
   },
   "id": "a2522d285d19c8ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        #Couche dense qui aura pour entrée la sortie du GRU\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        # Dernière couche dense\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # L'attention est defini par un modèle a part\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # Passage du mot courant à la couche embedding\n",
    "        x = self.embedding(x)\n",
    "        # Concaténation\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Passage du vecteur concaténé à la gru\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # Couche dense\n",
    "        y = self.fc1(output)\n",
    "\n",
    "        y = tf.reshape(y, (-1, x.shape[2]))\n",
    "\n",
    "        # Couche dense\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.843488400Z"
    }
   },
   "id": "58ef424e50efad84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Combiner la partie encodeur et décodeur :**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68bd1950bf0921de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création de l'encodeur\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "# Création du décodeur\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.845470700Z"
    }
   },
   "id": "6ff4994cc529948a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optimiseur ADAM\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# La fonction de perte\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.847470Z"
    }
   },
   "id": "b21b2d4647a794b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour garder la trace de notre apprentissage et la sauvegarder, on va utiliser la classe [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40abb86efead653e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.abspath(\"E:\\\\New folder\")\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.847470Z"
    }
   },
   "id": "a3b2eef3e1809fdc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialisation de l'époque de début d’entrainement dans `start_epoch`. La classe `tf.train.Checkpoint` permet de poursuivre l’entrainement là ou vous l’avez laissé s’il avait été interrompu auparavant."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96e8ef81f6828763"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # Restaurer le dernier checkpoint dans checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.849471200Z"
    }
   },
   "id": "7fbc3649e72526ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Entrainement et test :\n",
    "# 3.1 Entrainement"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ec8dc0626347584"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # Initialisation de l'état caché pour chaque batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    # Initialiser l'entrée du décodeur\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape: # Offre la possibilité de calculer le gradient du loss\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Prédiction des i'èmes mot du batch avec le décodeur\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Le mot correct à l'étap i est donné en entrée à l'étape (i+1)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.852502800Z"
    }
   },
   "id": "11fa30cf8095069f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le code global contenant la boucle d'entrainement est présenté ci-dessous. Cette boucle parcours le jeu de données d'entrainement batch par batch et entraine le réseaux avec ceux-ci.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86c509565a7b094c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # sauvegarde de la perte\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "# Affichage de la courbe d'entrainement\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.852502800Z"
    }
   },
   "id": "543b1fdc111a7395"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.2 Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab73ac53c58c555b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "\n",
    "# Fonction permettant la représentation de l'attention au niveau de l'image\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:36:02.930464900Z",
     "start_time": "2023-10-25T09:36:02.855472300Z"
    }
   },
   "id": "c479ead4f70edb69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'affichage de quelques exemples sur le résultat retourné par l'évaluation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41908be047f422fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Affichage de quelques annotations dans le jeu de test\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "print(rid)\n",
    "print(len(cap_val))\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.857474200Z"
    }
   },
   "id": "4c24de6350377840"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Image.open(img_name_val[rid])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T09:36:02.857474200Z"
    }
   },
   "id": "93a6b76bb9f13e76"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
