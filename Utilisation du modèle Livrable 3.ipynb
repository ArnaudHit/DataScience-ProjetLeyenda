{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1-Import des bibliothèques"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "915a4ca1a7b4e4e6"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:52:42.001186100Z",
     "start_time": "2023-10-25T14:52:41.984140500Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import collections\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2-Création d'un encodeur et décodeur avec une architecture identique au livrable 3\n",
    "Paramètres pour le modèle :"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0257846c65aa36"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 512 # Taille de la couche cachée dans le RNN\n",
    "top_k = 5000 # Nombre de mots à utiliser dans le vocabulaire\n",
    "vocab_size = top_k + 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:21:13.116298100Z",
     "start_time": "2023-10-25T14:21:13.111237900Z"
    }
   },
   "id": "431e52a1495d171d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1-Création de l'encodeur"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c52240258012f2db"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "    # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # forme après fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:21:14.620693200Z",
     "start_time": "2023-10-25T14:21:14.616452500Z"
    }
   },
   "id": "f1b198d325a13d8a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2-Création du décodeur"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51166b1d04983b95"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) forme == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # forme de la couche cachée == (batch_size, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # Cela vous donne un score non normalisé pour chaque caractéristique de l'image.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        #Couche dense qui aura pour entrée la sortie du GRU\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        # Dernière couche dense\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # L'attention est defini par un modèle a part\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # Passage du mot courant à la couche embedding\n",
    "        x = self.embedding(x)\n",
    "        # Concaténation\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Passage du vecteur concaténé à la gru\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # Couche dense\n",
    "        y = self.fc1(output)\n",
    "\n",
    "        y = tf.reshape(y, (-1, x.shape[2]))\n",
    "\n",
    "        # Couche dense\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:21:16.647788500Z",
     "start_time": "2023-10-25T14:21:16.647282100Z"
    }
   },
   "id": "d0d0384106b763df"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 14:21:17.451166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-25 14:21:17.459974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-25 14:21:17.460008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-25 14:21:17.460828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-25 14:21:17.460861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-25 14:21:17.460882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-25 14:21:18.638043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-25 14:21:18.638095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-25 14:21:18.638103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-25 14:21:18.638134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-25 14:21:18.638151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2848 MB memory:  -> device: 0, name: Quadro P4000, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Création de l'encodeur\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "# Création du décodeur\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:21:20.671903800Z",
     "start_time": "2023-10-25T14:21:17.404214800Z"
    }
   },
   "id": "bb4f8e13b6c6358d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3-Initialisation du gestionnaire de checkpoints"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43822181d54f7c4e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.abspath(\"checkpoints/\")  # Modifiez ce chemin si nécessaire.\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=tf.keras.optimizers.Adam())  # Assurez-vous d'utiliser les mêmes paramètres d'optimiseur que l'original.\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:55.149000200Z",
     "start_time": "2023-10-25T14:22:55.117370300Z"
    }
   },
   "id": "65db95b28cf54c34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4-Restaurez le dernier checkpoint"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7347a3dc316aa8a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dernier checkpoint restauré !\n"
     ]
    }
   ],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Dernier checkpoint restauré !\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:56.876408Z",
     "start_time": "2023-10-25T14:22:56.848733300Z"
    }
   },
   "id": "f1dbc13c652a0c10"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5-Utilisation du modèle sur une image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a30b5d4a81993c7e"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Charge et prétraite l'image pour le modèle.\n",
    "    \n",
    "    Args:\n",
    "    - image_path (str): Chemin vers l'image.\n",
    "    \n",
    "    Returns:\n",
    "    - tf.Tensor: Image tensor prétraitée.\n",
    "    \"\"\"\n",
    "    # Chargez l'image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:36:08.972420Z",
     "start_time": "2023-10-25T14:36:08.963438400Z"
    }
   },
   "id": "df6de9585cef3284"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def generate_caption(image_path, encoder, decoder, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Génère une légende pour l'image donnée.\n",
    "    \n",
    "    Args:\n",
    "    - image_path (str): Chemin vers l'image.\n",
    "    - encoder (tf.keras.Model): Le modèle encodeur.\n",
    "    - decoder (tf.keras.Model): Le modèle décodeur.\n",
    "    - tokenizer (tf.keras.preprocessing.text.Tokenizer): Le tokenizer utilisé pour le prétraitement des légendes.\n",
    "    - max_length (int): La longueur maximale d'une légende.\n",
    "    \n",
    "    Returns:\n",
    "    - str: La légende générée pour l'image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Chargez et pré-traitez l'image\n",
    "    temp_input = tf.expand_dims(load_and_preprocess_image(image_path), 0)\n",
    "    img_tensor_val = encoder(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    # Initialisez l'entrée du décodeur avec le jeton de départ\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    # Réinitialisez l'état caché du décodeur\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden = decoder(dec_input, img_tensor_val, hidden)\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    caption = ' '.join(result).replace('<start>', '').replace('<end>', '').strip()\n",
    "    return caption\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:36:21.286729300Z",
     "start_time": "2023-10-25T14:36:21.277669800Z"
    }
   },
   "id": "8ea989faf094b78c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON string from the file\n",
    "with open('tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "\n",
    "# Recreate the tokenizer from the JSON string\n",
    "tokenizer = tokenizer_from_json(tokenizer_json)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:52:53.222105600Z",
     "start_time": "2023-10-25T14:52:53.193157100Z"
    }
   },
   "id": "2ceb5e3849c606ae"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'cnn__encoder' (type CNN_Encoder).\n\nIn this `tf.Variable` creation, the initial value's shape ((2048, 256)) is not compatible with the explicitly supplied `shape` argument ((3, 256)).\n\nCall arguments received by layer 'cnn__encoder' (type CNN_Encoder):\n  • x=tf.Tensor(shape=(1, 299, 299, 3), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 9\u001B[0m\n\u001B[1;32m      5\u001B[0m rid \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(image_file))\n\u001B[1;32m      7\u001B[0m image_test \u001B[38;5;241m=\u001B[39m image_file[rid]\n\u001B[0;32m----> 9\u001B[0m caption \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_caption\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m                           \u001B[49m\u001B[38;5;241;43m47\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLégende générée:\u001B[39m\u001B[38;5;124m\"\u001B[39m, caption)\n\u001B[1;32m     16\u001B[0m Image\u001B[38;5;241m.\u001B[39mopen(image_test)\n",
      "Cell \u001B[0;32mIn[15], line 18\u001B[0m, in \u001B[0;36mgenerate_caption\u001B[0;34m(image_path, encoder, decoder, tokenizer, max_length)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Chargez et pré-traitez l'image\u001B[39;00m\n\u001B[1;32m     17\u001B[0m temp_input \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mexpand_dims(load_and_preprocess_image(image_path), \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 18\u001B[0m img_tensor_val \u001B[38;5;241m=\u001B[39m \u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtemp_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m img_tensor_val \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreshape(img_tensor_val, (img_tensor_val\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, img_tensor_val\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m3\u001B[39m]))\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Initialisez l'entrée du décodeur avec le jeton de départ\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Cell \u001B[0;32mIn[5], line 10\u001B[0m, in \u001B[0;36mCNN_Encoder.call\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 10\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     x \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[0;31mValueError\u001B[0m: Exception encountered when calling layer 'cnn__encoder' (type CNN_Encoder).\n\nIn this `tf.Variable` creation, the initial value's shape ((2048, 256)) is not compatible with the explicitly supplied `shape` argument ((3, 256)).\n\nCall arguments received by layer 'cnn__encoder' (type CNN_Encoder):\n  • x=tf.Tensor(shape=(1, 299, 299, 3), dtype=float32)"
     ]
    }
   ],
   "source": [
    "image_folder = \"./photo/Photo/\"\n",
    "\n",
    "image_file = [os.path.join(image_folder, filename) for filename in os.listdir(image_folder) if filename.endswith('.jpg')]\n",
    "\n",
    "rid = np.random.randint(0, len(image_file))\n",
    "\n",
    "image_test = image_file[rid]\n",
    "\n",
    "caption = generate_caption(image_test, \n",
    "                           encoder, \n",
    "                           decoder, \n",
    "                           tokenizer, \n",
    "                           47)\n",
    "print(\"Légende générée:\", caption)\n",
    "\n",
    "Image.open(image_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:54:06.055832200Z",
     "start_time": "2023-10-25T14:54:05.980573300Z"
    }
   },
   "id": "c8f683b16dd612e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "744d68c622145ffd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
