{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Projet LEYENDA\n",
    "## Livrable 1 - Classification Binaire"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bbcdab41f7ad26c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "#dir = './' Chemin pour le tree"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T14:36:14.726025200Z",
     "start_time": "2023-10-03T14:36:14.721174700Z"
    }
   },
   "id": "604acc0424d31276"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjour\n"
     ]
    }
   ],
   "source": [
    "print(\"bonjour\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T15:05:28.610946200Z",
     "start_time": "2023-10-03T15:05:28.591816900Z"
    }
   },
   "id": "6a456b86cae5094b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 15:05:32.263385: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-03 15:05:32.263703: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-03 15:05:32.263732: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-03 15:05:32.270126: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 15:05:34.520976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-03 15:05:34.525107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-03 15:05:34.525143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T15:05:34.528956700Z",
     "start_time": "2023-10-03T15:05:31.976283Z"
    }
   },
   "id": "5af3c0018147b05f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Importation des bibliothèques"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7705d02550f9b2d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T15:05:38.072092900Z",
     "start_time": "2023-10-03T15:05:37.618527400Z"
    }
   },
   "id": "4ff84e3050962ffd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Préparation des images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab9c6473dd452b67"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier Dataset Livrable 1 - Photo.zip décompressé dans le dossier Dataset.\n",
      "Fichier Dataset Livrable 1 - Text.zip décompressé dans le dossier Dataset.\n",
      "Fichier Dataset Livrable 1 - Sketch.zip décompressé dans le dossier Dataset.\n",
      "Fichier Dataset Livrable 1 - Schematics.zip décompressé dans le dossier Dataset.\n",
      "Tous les fichiers ZIP ont été décompressés dans le dossier Dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "repertoire = './'\n",
    "dossier_dataset = './Data'\n",
    "if not os.path.exists(dossier_dataset):\n",
    "    os.makedirs(dossier_dataset)\n",
    "    \n",
    "for fichier in os.listdir(repertoire):\n",
    "    chemin_fichier = os.path.join(repertoire,fichier)\n",
    "    if zipfile.is_zipfile(chemin_fichier):\n",
    "        with zipfile.ZipFile(chemin_fichier, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dossier_dataset)\n",
    "            print(f'Fichier {fichier} décompressé dans le dossier Dataset.')\n",
    "print('Tous les fichiers ZIP ont été décompressés dans le dossier Dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T15:09:03.420328100Z",
     "start_time": "2023-10-03T15:08:21.550577100Z"
    }
   },
   "id": "f5ec94da62cf051e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "datapath = \"./Data/\"\n",
    "data_dir = pathlib.Path(datapath)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T15:10:21.356787400Z",
     "start_time": "2023-10-03T15:10:21.352392600Z"
    }
   },
   "id": "55e8603cf19c764c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "image_h = 360\n",
    "image_w = 360\n",
    "batch_s = 32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T15:10:21.904570Z",
     "start_time": "2023-10-03T15:10:21.899579600Z"
    }
   },
   "id": "2421b5f437d9cb68"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "image_dataset_from_directory() missing 1 required positional argument: 'directory'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Le train_set\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m train_set \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocessing\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_dataset_from_directory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./Data\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43msubset\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimage_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_w\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_s\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Le test_set\u001B[39;00m\n\u001B[1;32m     11\u001B[0m test_set \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mpreprocessing\u001B[38;5;241m.\u001B[39mimage_dataset_from_directory(\n\u001B[1;32m     12\u001B[0m     data_dir,\n\u001B[1;32m     13\u001B[0m     validation_split\u001B[38;5;241m=\u001B[39m  \u001B[38;5;241m0.2\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     17\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_s\n\u001B[1;32m     18\u001B[0m )\n",
      "\u001B[0;31mTypeError\u001B[0m: image_dataset_from_directory() missing 1 required positional argument: 'directory'"
     ]
    }
   ],
   "source": [
    "# Le train_set\n",
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir = \"./Data\",\n",
    "    validation_split=  0.2,\n",
    "    subset =  \"training\",\n",
    "    seed=42,\n",
    "    image_size=(image_h, image_w),\n",
    "    batch_size=batch_s\n",
    ")\n",
    "# Le test_set\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=  0.2,\n",
    "    subset =  \"validation\",\n",
    "    seed=42,\n",
    "    image_size=(image_h, image_w),\n",
    "    batch_size=batch_s\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T15:10:26.853516Z",
     "start_time": "2023-10-03T15:10:26.833821200Z"
    }
   },
   "id": "ebeaf5921868e37c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_names = train_set.class_names\n",
    "print(class_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.928559500Z"
    }
   },
   "id": "ae9a6d420817ad65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax =  plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.928559500Z"
    }
   },
   "id": "7de352088c714a3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(type(train_set))\n",
    "images, labels =  next(iter(train_set))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.928559500Z"
    }
   },
   "id": "1877bff46b0ad885"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_set = train_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_set = test_set.cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.933310700Z"
    }
   },
   "id": "f9e192d688c4f87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = 5 # Nombre de classes et donc aussi nombre de neurones dans la dernière couche\n",
    "model = Sequential()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.933310700Z"
    }
   },
   "id": "dadbd60a8636b643"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.add(layers.experimental.preprocessing.Rescaling(1./255))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.933310700Z"
    }
   },
   "id": "cbebbf44b9649bd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Couche de convolution\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "# Couche de pooling\n",
    "model.add(layers.MaxPooling2D((2, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.933310700Z"
    }
   },
   "id": "cfd0928113c49f5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bloc convolutif ou la taille du filtre est de (32, 3)\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "# Bloc convolutif ou la taille du filtre est de (64, 3)\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "# Applatissement de la couche\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Couche entièrement connectée (couche dense)\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "# Couche entièrement connectée retournant le résultat de la classification\n",
    "model.add(layers.Dense(num_classes))\n",
    "\n",
    "model.build((None, image_h, image_w, 3))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.933310700Z"
    }
   },
   "id": "c31dfe85feb177eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer =  'adam',\n",
    "              loss =  tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.933310700Z"
    }
   },
   "id": "e8bc0dd3cecd81d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.src.layers.preprocessing.image_preprocessing import HORIZONTAL_AND_VERTICAL\n",
    "from keras.src.layers.preprocessing.image_preprocessing import RandomFlip\n",
    "from keras.src.layers.preprocessing.image_preprocessing import RandomRotation\n",
    "from keras.src.layers.preprocessing.image_preprocessing import RandomZoom\n",
    "from keras import Sequential\n",
    "\n",
    "data_augmentation = Sequential(\n",
    "    [\n",
    "        RandomFlip(\n",
    "            mode=HORIZONTAL_AND_VERTICAL,\n",
    "            input_shape=(image_h, image_w, 3)),\n",
    "        RandomRotation(\n",
    "            factor=0.18,\n",
    "            fill_mode='reflect',\n",
    "            interpolation='bilinear',\n",
    "            seed=None,\n",
    "            fill_value=0.0),\n",
    "        RandomZoom(\n",
    "            height_factor=0.1,\n",
    "            width_factor=None,\n",
    "            fill_mode='reflect',\n",
    "            interpolation='bilinear',\n",
    "            seed=None,\n",
    "            fill_value=0.0)\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.933310700Z"
    }
   },
   "id": "d959b2e8aa790225"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Le modèle\n",
    "epochs = 8\n",
    "complete_model =  Sequential([\n",
    "    layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes)\n",
    "])\n",
    "complete_model.build((None, image_h, image_w, 3))\n",
    "# Compilation du modèle\n",
    "complete_model.compile(optimizer =  'adam',\n",
    "                       loss =  tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=['accuracy'])\n",
    "# Résumé du modèle\n",
    "complete_model.summary()\n",
    "# Enrainement du modèle\n",
    "history =  complete_model.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs\n",
    ")\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.933310700Z"
    }
   },
   "id": "c1934c7ccdfd3ac2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T13:37:35.933310700Z"
    }
   },
   "id": "976d5e14d0cf651"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
